{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Graph-Based Approach for Item Recommendations<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this approach is to convert the transactional data into a weighted graph, and mine it to find suitable substitutes for any given product. Based on our research, [GraphSWAG](https://arxiv.org/pdf/1911.10232.pdf) is a good algorithm for mining transactional graphs.\n",
    "\n",
    "GraphSWAG is a Graph Convolutional Network (GCN) developed by researchers at Target that combines sales data, product descriptions and product images to generate node embeddings for large graphs. Since we did not have access to the data used by Target, we had to make modifications to the algorithm and try to implement it on our own.\n",
    "\n",
    "<img src=\"notebook_image/shopping_graph.png\" alt=\"shopping_graph\" style=\"width: 325px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from datetime import datetime as dt\n",
    "from datetime import date\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from sklearn.neighbors import NearestNeighbors as NN\n",
    "from numpy.random import choice\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_transaction_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_num</th>\n",
       "      <th>qty_sold</th>\n",
       "      <th>item_price</th>\n",
       "      <th>qty_is_weight</th>\n",
       "      <th>ticket_num</th>\n",
       "      <th>date</th>\n",
       "      <th>time_scanned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4889</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>2527</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:03:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>6</td>\n",
       "      <td>460</td>\n",
       "      <td>429</td>\n",
       "      <td>1</td>\n",
       "      <td>2527</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:03:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2528</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2528</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7013201045</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>2529</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:05:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   global_transaction_id     item_id  dept_num  qty_sold  item_price  \\\n",
       "0                      0        4889         6         3          99   \n",
       "1                      0        3125         6       460         429   \n",
       "2                      1           5        20         4         100   \n",
       "3                      1           2        20         6         100   \n",
       "4                      2  7013201045         1         1         299   \n",
       "\n",
       "   qty_is_weight  ticket_num        date time_scanned  \n",
       "0              0        2527  2020-07-01     07:03:30  \n",
       "1              1        2527  2020-07-01     07:03:34  \n",
       "2              0        2528  2020-07-01     07:04:22  \n",
       "3              0        2528  2020-07-01     07:04:25  \n",
       "4              0        2529  2020-07-01     07:05:06  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transactions\n",
    "\n",
    "ncr_transactions =  pd.read_csv('../raw_data/ncr/items_transactions.csv')\n",
    "ncr_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>description</th>\n",
       "      <th>ecomm_description</th>\n",
       "      <th>category</th>\n",
       "      <th>item_type</th>\n",
       "      <th>upc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAN DULCE SENCILLO</td>\n",
       "      <td>Mexican Sweet Bread/Pan Dulce Mexicano, 1 Count</td>\n",
       "      <td>20101020</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BOLILLO FRENCH ROLLS</td>\n",
       "      <td>Bolillo, French Rolls, 1 Count</td>\n",
       "      <td>20101210</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>BOLILLO QUESO/CHILE JALAP</td>\n",
       "      <td>Jalapeño and Cheese Bolillo, 1 Count</td>\n",
       "      <td>20101210</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>EMPANADA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20101020</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MIni Bolillo</td>\n",
       "      <td>BOLILLO SMALL, 2 OZ</td>\n",
       "      <td>20101210</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                description  \\\n",
       "0        1         PAN DULCE SENCILLO   \n",
       "1        2       BOLILLO FRENCH ROLLS   \n",
       "2        3  BOLILLO QUESO/CHILE JALAP   \n",
       "3        4                   EMPANADA   \n",
       "4        5               MIni Bolillo   \n",
       "\n",
       "                                 ecomm_description  category  item_type  upc  \n",
       "0  Mexican Sweet Bread/Pan Dulce Mexicano, 1 Count  20101020          0   10  \n",
       "1                   Bolillo, French Rolls, 1 Count  20101210          0   20  \n",
       "2             Jalapeño and Cheese Bolillo, 1 Count  20101210          0   30  \n",
       "3                                              NaN  20101020          0   40  \n",
       "4                              BOLILLO SMALL, 2 OZ  20101210          0   50  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Items\n",
    "\n",
    "ncr_items =  pd.read_csv('../raw_data/ncr/items_descriptions.csv')\n",
    "ncr_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Removing nonrelevant products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>description</th>\n",
       "      <th>ecomm_description</th>\n",
       "      <th>category</th>\n",
       "      <th>item_type</th>\n",
       "      <th>upc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pan dulce sencillo mexican sweet bread pan dul...</td>\n",
       "      <td>Mexican Sweet Bread/Pan Dulce Mexicano, 1 Count</td>\n",
       "      <td>20101020</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bolillo french rolls bolillo french rolls 1 count</td>\n",
       "      <td>Bolillo, French Rolls, 1 Count</td>\n",
       "      <td>20101210</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bolillo queso chile jalap jalapeo and cheese b...</td>\n",
       "      <td>Jalapeño and Cheese Bolillo, 1 Count</td>\n",
       "      <td>20101210</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>empanada</td>\n",
       "      <td></td>\n",
       "      <td>20101020</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>mini bolillo bolillo small 2 oz</td>\n",
       "      <td>BOLILLO SMALL, 2 OZ</td>\n",
       "      <td>20101210</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                        description  \\\n",
       "0        1  pan dulce sencillo mexican sweet bread pan dul...   \n",
       "1        2  bolillo french rolls bolillo french rolls 1 count   \n",
       "2        3  bolillo queso chile jalap jalapeo and cheese b...   \n",
       "3        4                                           empanada   \n",
       "4        5                    mini bolillo bolillo small 2 oz   \n",
       "\n",
       "                                 ecomm_description  category  item_type  upc  \n",
       "0  Mexican Sweet Bread/Pan Dulce Mexicano, 1 Count  20101020          0   10  \n",
       "1                   Bolillo, French Rolls, 1 Count  20101210          0   20  \n",
       "2             Jalapeño and Cheese Bolillo, 1 Count  20101210          0   30  \n",
       "3                                                   20101020          0   40  \n",
       "4                              BOLILLO SMALL, 2 OZ  20101210          0   50  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_transaction_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_num</th>\n",
       "      <th>qty_sold</th>\n",
       "      <th>item_price</th>\n",
       "      <th>qty_is_weight</th>\n",
       "      <th>ticket_num</th>\n",
       "      <th>date</th>\n",
       "      <th>time_scanned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4889</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>2527</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:03:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3125</td>\n",
       "      <td>6</td>\n",
       "      <td>460</td>\n",
       "      <td>429</td>\n",
       "      <td>1</td>\n",
       "      <td>2527</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:03:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2528</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>2528</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:04:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7013201045</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>2529</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>07:05:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   global_transaction_id     item_id  dept_num  qty_sold  item_price  \\\n",
       "0                      0        4889         6         3          99   \n",
       "1                      0        3125         6       460         429   \n",
       "2                      1           5        20         4         100   \n",
       "3                      1           2        20         6         100   \n",
       "4                      2  7013201045         1         1         299   \n",
       "\n",
       "   qty_is_weight  ticket_num        date time_scanned  \n",
       "0              0        2527  2020-07-01     07:03:30  \n",
       "1              1        2527  2020-07-01     07:03:34  \n",
       "2              0        2528  2020-07-01     07:04:22  \n",
       "3              0        2528  2020-07-01     07:04:25  \n",
       "4              0        2529  2020-07-01     07:05:06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Removing product id's from nonrelevant products, such as paper bags, plastic bag tax refunds, etc...\n",
    "\n",
    "idsToRemove = [9492206955, 5555, 75,7501,7502,7503,7504,7505,7506,7507,7508,7509,7510,7511,7512,7513,7514,7515,7516,7517,7518,\n",
    "7519,7520,7521,7522,7523,7524,7525,7526,7527,7528,7529,7530,7531,7532,7533,7535,7536,7537,7538,7539,7540,\n",
    "7541,7542,7543,7544,7545,7546,7547,7548,7549,7550,8771,8772,9999910012]\n",
    "\n",
    "ncr_items = ncr_items[~ncr_items.item_id.isin(idsToRemove)]\n",
    "ncr_items = ncr_items.replace(np.nan, '', regex=True)\n",
    "\n",
    "ncr_items[\"description\"] = ncr_items[\"description\"].map(str) + ' ' + ncr_items[\"ecomm_description\"].map(str)\n",
    "ncr_items[\"description\"] = ncr_items[\"description\"].str.rstrip(' ')\n",
    "ncr_items[\"description\"] = ncr_items[\"description\"].str.lower().str.replace('/',' ').str.replace('-',' ').replace(r'[^A-Za-z0-9. ]+', '', regex=True)\n",
    "\n",
    "ncr_transactions = ncr_transactions[~ncr_transactions.item_id.isin(idsToRemove)]\n",
    "display(ncr_items.head())\n",
    "display(ncr_transactions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Saving the item ids and item names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the item ids and item names as single arrays\n",
    "\n",
    "item_ids = np.asarray(ncr_items[\"item_id\"])\n",
    "item_names = np.asarray(ncr_items[\"description\"].str.split())\n",
    "\n",
    "# Saving the arrays to be quickly loaded if needed\n",
    "\n",
    "np.save('../raw_data/model_outputs/item_ids.npy', item_ids)\n",
    "np.save('../raw_data/model_outputs/item_names.npy', item_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre-saved item ids and item names arrays\n",
    "\n",
    "item_ids = np.load('../raw_data/model_outputs/item_ids.npy', allow_pickle=True)\n",
    "item_names = np.load('../raw_data/model_outputs/item_names.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generating the Weighted Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Structure:**\n",
    "- **Nodes:** Individual Products\n",
    "    - Non-food items such as plastic bags were removed\n",
    "    - Items= ids without names will be removed\n",
    "- **Edges:** Weighted Pairwise Co-Occurrence\n",
    "    - Weights are calculated by adding co-occurrent transactions adjusted by **time-decay** and transforming them to a [0,1] range using the **arctan** function\n",
    " \n",
    "<br><br>  \n",
    "**Edge Weighting Function:**\n",
    "<img src=\"notebook_image/weighting_function.png\" alt=\"weighting_function\" style=\"width: 350px;\"/>\n",
    "<br><br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Converting the transactional dataset to a list of weighted edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction 1 of 123078\n",
      "Transaction 10001 of 123078\n",
      "Transaction 20001 of 123078\n",
      "Transaction 30001 of 123078\n",
      "Transaction 40001 of 123078\n",
      "Transaction 50001 of 123078\n",
      "Transaction 60001 of 123078\n",
      "Transaction 70001 of 123078\n",
      "Transaction 80001 of 123078\n",
      "Transaction 90001 of 123078\n",
      "Transaction 100001 of 123078\n",
      "Transaction 110001 of 123078\n",
      "Transaction 120001 of 123078\n",
      "Transaction 123079 of 123078\n"
     ]
    }
   ],
   "source": [
    "graph_dict = defaultdict(int)\n",
    "transaction_ids = ncr_transactions.global_transaction_id.unique()\n",
    "today = date.today()\n",
    "\n",
    "# Looping through unique transaction ids\n",
    "for id in transaction_ids:\n",
    "    if id % 10000 == 0 or id == 0 or id == len(transaction_ids):\n",
    "        print('Transaction ' + str(id + 1) + ' of ' + str(len(transaction_ids)))\n",
    "    \n",
    "    # Looping through each unique transaction\n",
    "    transaction = ncr_transactions[ncr_transactions.global_transaction_id == id]\n",
    "    for i in range(len(transaction) - 1):\n",
    "        for j in range(i,len(transaction)):\n",
    "            pair = (transaction.iloc[i].item_id,transaction.iloc[j].item_id)\n",
    "            if not isinstance(pair,int):\n",
    "                \n",
    "                # Checking if both product ids are different\n",
    "                if len(set(pair)) == len(pair):\n",
    "                    \n",
    "                    # Adding co-occurance weight, applying time decay \n",
    "                    transaction_date = dt.strptime(transaction.iloc[i].date, '%Y-%m-%d').date()\n",
    "                    delta = today - transaction_date\n",
    "                    graph_dict[pair] += 1/delta.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the graph dictionary as a list of lists\n",
    "graph_list = []\n",
    "\n",
    "for pair in graph_dict:\n",
    "    if not isinstance(pair,int):\n",
    "        row = list(pair)\n",
    "        row.append(graph_dict[pair])\n",
    "        graph_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_a</th>\n",
       "      <th>product_b</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_a  product_b    weight\n",
       "0          1          2  0.026733\n",
       "1          1          3  0.001332\n",
       "2          1          5  0.005448\n",
       "3          1          6  0.000069\n",
       "4          1          9  0.000080"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the graph list into a Pandas DataFrame\n",
    "graph_df = pd.DataFrame(data = graph_list, columns=['product_a', 'product_b', 'weight'])\n",
    "\n",
    "# Grouping the DataFrame by product pairs\n",
    "graph_df = graph_df.groupby(['product_a', 'product_b'])[['weight']].sum()\n",
    "\n",
    "# Normalizing the graph weights\n",
    "weight_sums = graph_df.groupby('product_a')['weight'].sum()\n",
    "graph_df.weight = graph_df.weight / weight_sums\n",
    "graph_df = graph_df.reset_index()\n",
    "\n",
    "\n",
    "# Applying the Arctan function to the weight values\n",
    "graph_df['weight'] = np.arctan(graph_df['weight'])\n",
    "\n",
    "graph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the edge list DataFrame as a CSV file\n",
    "graph_df.to_csv('../raw_data/model_outputs/graph_edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the edge list graph as a Pandas DataFrame\n",
    "graph_df = pd.read_csv('../raw_data/model_outputs/graph_edges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Converting the edge list into an adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a NetworkX object from the edge list DataFrame\n",
    "graph = nx.from_pandas_edgelist(graph_df, 'product_a', 'product_b', edge_attr='weight')\n",
    "graph.name = 'Product Transactions Graph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Name: Product Transactions Graph<br>Type: Graph<br>Number of nodes: 10047<br>Number of edges: 2405295<br>Average degree: 478.8086"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Printing information about the graph\n",
    "graph_info = nx.info(graph)\n",
    "graph_info = graph_info.replace('\\n','<br>')\n",
    "display(HTML(graph_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10047, 10047)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exporting the graph as a numpy adjacency matrix\n",
    "adj_matrix = nx.to_numpy_matrix(graph)\n",
    "adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the adjacency matrix as a npy file\n",
    "np.save('../raw_data/model_outputs/adj_matrix.npy', adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = np.load('../raw_data/model_outputs/adj_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10047, 10047)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the adjacency matrix to a CSR sparse matrix to save memory\n",
    "sparse_adj_matrix = csr_matrix(adj_matrix)\n",
    "sparse_adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the sparse adjacency matrix as an npz file\n",
    "save_npz('../raw_data/model_outputs/sparse_adj_matrix.npz', sparse_adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10047, 10047)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_adj_matrix = load_npz(\"../raw_data/model_outputs/sparse_adj_matrix.npz\")\n",
    "sparse_adj_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loading the Word Embedding Vectors generated using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48496, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_vectors = np.load(\"../raw_data/model_outputs/item_vectors.npy\")\n",
    "item_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Matching the Product IDs in the Edge Graph and Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping word embedding vectors from item ids that were not present in the transactional dataset\n",
    "\n",
    "unique_ids = sorted(graph_df['product_a'].append(graph_df['product_b']).unique())\n",
    "\n",
    "vector_df = pd.DataFrame(data = item_ids, columns = ['item_id'])\n",
    "vector_df['vector'] = [item_vectors[i] for i in range(len(item_vectors))]\n",
    "vector_df = vector_df[vector_df.item_id.isin(unique_ids)].reset_index(drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping graph edges with product ids that were not present in the item names dataset\n",
    "\n",
    "vector_unique_ids = vector_df.item_id.unique()\n",
    "\n",
    "graph_df = graph_df[graph_df.product_a.isin(vector_unique_ids)]\n",
    "graph_df = graph_df[graph_df.product_b.isin(vector_unique_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned edge list DataFrame as a CSV file\n",
    "graph_df.to_csv('../raw_data/model_outputs/graph_edges_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9722, 9722)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-creating the NX graph object\n",
    "graph = nx.from_pandas_edgelist(graph_df, 'product_a', 'product_b', edge_attr='weight')\n",
    "\n",
    "# Re-creating the adjacency matrix\n",
    "adj_matrix = nx.to_numpy_matrix(graph)\n",
    "\n",
    "# Converting it to a CSR sparse matrix\n",
    "sparse_adj_matrix = csr_matrix(adj_matrix)\n",
    "sparse_adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleaned sparse adjacency matrix as an npz file\n",
    "save_npz('../raw_data/model_outputs/sparse_adj_matrix_clean.npz', sparse_adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Graph Convolutional Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Convolutional Network Steps:**\n",
    "- Sampling:\n",
    "    - K neighbors of each node are sampled before each layer.\n",
    "    - A beta hyperparameter assign a higher or lower importance to the edge weights when generating sampling probabilities.\n",
    "- Aggregation:\n",
    "    - The feature vectors of all sampled neighbors of a node a combined with some aggregation function.\n",
    "    - We used the Mean aggregator, similarly to the Target paper.\n",
    "- Concatenation:\n",
    "    - Each aggregated feature vector is concatenated to the output of the previous layer before being input to the next layer.\n",
    "- Non-Linear Activation Function:\n",
    "    - All hidden layers used the ReLu activation funciton\n",
    "    - The final layer used the Softmax activation funciton\n",
    "- Final Output:\n",
    "    - 200-long feature vectors embedding each node of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Convolutional Network (GCN) Architecture**:\n",
    "<img src=\"notebook_image/gcn.png\" alt=\"gcn\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Sampling & Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling and Aggregation Algorithms**:\n",
    "<br>\n",
    "<img src=\"notebook_image/s_a.png\" alt=\"s_a\" style=\"width: 750px;\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_neighbors(a_series, beta, k):\n",
    "    all_weights = a_series.to_numpy()\n",
    "    nonzero_indices = np.nonzero(all_weights)[0]\n",
    "    nonzero_weights = np.power(all_weights[np.nonzero(all_weights)],beta) * k\n",
    "    nonzero_weights /= sum(nonzero_weights)\n",
    "\n",
    "    neighbors = np.random.choice(nonzero_indices, \n",
    "                                 min(k,len(np.nonzero(nonzero_weights)[0])), \n",
    "                                 p=nonzero_weights, replace=False)\n",
    "    weights = all_weights[neighbors]\n",
    "    return [(neighbors[i], weights[i]) for i in range(0, len(neighbors))] \n",
    "\n",
    "def expand_graph(graph_df):\n",
    "    graph_df[\"u_adj_idx\"] = np.arange(sparse_adj_matrix.shape[0])\n",
    "    graph_df = graph_df.explode(\"neighbors and weights\")\n",
    "    graph_df[\"v_adj_idx\"] = graph_df[\"neighbors and weights\"].apply(lambda x: x[0])\n",
    "    graph_df[\"weight\"] = graph_df[\"neighbors and weights\"].apply(lambda x: x[1])\n",
    "    return graph_df[[\"u_adj_idx\",\"v_adj_idx\",\"weight\"]]\n",
    "\n",
    "def add_vectors_column(graph_df, vectors_df):\n",
    "    graph_df = graph_df.merge(vectors_df.item_id, how = 'inner', left_on = 'u_adj_idx', right_index = True)\n",
    "    graph_df = graph_df.merge(vectors_df, how = 'inner', left_on = 'v_adj_idx', right_index = True)\n",
    "    graph_df.columns = ['u_adj_idx', 'v_adj_idx', 'weight', 'u_id', 'v_id', 'vector']\n",
    "    graph_df = graph_df.sort_values(by=['u_adj_idx', 'v_adj_idx'])\n",
    "    return graph_df\n",
    "\n",
    "\n",
    "def sampling(sparse_adj_matrix, vectors_df, beta = 10, k = 10):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    w = pd.DataFrame(sparse_adj_matrix.toarray())\n",
    "    w[\"neighbors and weights\"] = w.apply(lambda a_series: sampling_neighbors(a_series, beta, k), axis=1)\n",
    "    return expand_graph(w[[\"neighbors and weights\"]])\n",
    "\n",
    "def sampling_and_aggregating(sparse_adj_matrix, prev_vectors, beta = 10, k = 10, gamma = 10):\n",
    "    \n",
    "    this_graph = sampling(sparse_adj_matrix, vector_df)\n",
    "    this_graph = add_vectors_column(this_graph, prev_vectors)\n",
    "\n",
    "    aggregated = this_graph.groupby('u_adj_idx').vector.apply(lambda g: np.mean(g.values.tolist(), axis=0))\n",
    "    aggregated = aggregated.apply(lambda g: g/np.linalg.norm(g))\n",
    "\n",
    "    aggregated = [prev_vectors.vector.iloc[i].tolist() + aggregated.iloc[i].tolist() for i in range(len(prev_vectors))]\n",
    "    new_vectors = prev_vectors.copy()\n",
    "    new_vectors['vector'] = aggregated\n",
    "    \n",
    "    return np.matrix(aggregated, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building GCN Layers and Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def prepare_h0(vector_df):\n",
    "    h0 = vector_df.copy()\n",
    "    h0['vector'] = h0['vector'].apply(lambda g: g/np.linalg.norm(g))\n",
    "    return h0\n",
    "\n",
    "def relu(h):\n",
    "    return np.maximum(h, 0)\n",
    "\n",
    "def gcn_layer(layer_input, W, vector_df, activation='relu'):\n",
    "    h = vector_df.copy()\n",
    "    if activation == 'relu':\n",
    "        act_matrix = np.asarray(relu(layer_input * W))       \n",
    "    elif activation == 'softmax':\n",
    "        act_matrix  = np.asarray(softmax(layer_input * W))\n",
    "    \n",
    "    h['vector'] = [np.array(row) for row in act_matrix]\n",
    "    h['vector'] = h['vector'].apply(lambda g: g/np.linalg.norm(g))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Get Product Recommendations using the Nearest Neighbors Aglorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbors(z, n_neighbors=5):\n",
    "    \n",
    "    knn_model = NN(n_neighbors=n_neighbors)\n",
    "    knn_model.fit(z.vector.tolist())\n",
    "    \n",
    "    return knn_model.kneighbors_graph(z.vector.tolist()).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations_by_product(nn_matrix, vector_df, items_df, item_idx):\n",
    "    item = items_df[items_df['item_id'] == vector_df.iloc[item_idx].item_id].description.values[0]\n",
    "    print(\"Substitutes to \\\"\" + item + \"\\\":\")\n",
    "    print(items_df[items_df['item_id'].isin(vector_df.iloc[np.where(nn_matrix[item_idx])[0].tolist()].item_id.values)].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Initial trail run of the GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Randomly generating initial GCN weight parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = np.random.normal(\n",
    "    loc=0, scale=1, size=(400, 200))\n",
    "W_2 = np.random.normal(\n",
    "    loc=0, size=(400, 200))\n",
    "W_3 = np.random.normal(\n",
    "    loc=0, size=(400, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Inputting data through the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = prepare_h0(vector_df)\n",
    "\n",
    "layer_input = sampling_and_aggregating(sparse_adj_matrix, h0)\n",
    "h1 = gcn_layer(layer_input, W_1, vector_df)\n",
    "\n",
    "layer_input = sampling_and_aggregating(sparse_adj_matrix, h1)\n",
    "h2 = gcn_layer(layer_input, W_2, vector_df)\n",
    "\n",
    "layer_input = sampling_and_aggregating(sparse_adj_matrix, h2)\n",
    "z = gcn_layer(layer_input, W_3, vector_df, 'softmax')\n",
    "\n",
    "nn_matrix = get_nearest_neighbors(z,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Printing some recommendation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substitutes to \"coors light coors light 30pk 12 ounce\":\n",
      "3564                            lets celebrate arrangement\n",
      "8273     ah liquid 2 x conc clean burst arm  hammer liq...\n",
      "8571     reeses peanut butter cups reeses peanut butter...\n",
      "9964                  tide simply oxi refresh breeze liq d\n",
      "15535    anthy small shells anthonys small shells 16 ounce\n",
      "18626                coors light coors light 30pk 12 ounce\n",
      "Name: description, dtype: object\n",
      "\n",
      "Substitutes to \"maiz cancha amarillo maiz cancha amarillo 15 oz\":\n",
      "4489     floridas nat oj w pulp floridas nat oj w pulp ...\n",
      "9737     always quiltwd thin long always pads ultra thi...\n",
      "11498    martinelli sparkling apl grape martinelli spar...\n",
      "12996    yakisoba teriyaki maruchan teriyaki beef flavo...\n",
      "28462    bistec de res para la parrilla beef steak gril...\n",
      "38442      maiz cancha amarillo maiz cancha amarillo 15 oz\n",
      "Name: description, dtype: object\n",
      "\n",
      "Substitutes to \"knorr chicken rice side knorr rice sides rice sides dish chicken 5.6 oz\":\n",
      "4862                                  bud light 24 12 lnnr\n",
      "11030    knorr chicken rice side knorr rice sides rice ...\n",
      "11720     goya beans black w salt goya bean black 29 ounce\n",
      "14311    eg sesame seed natural el guapo natural sesame...\n",
      "23065    jack daniels fire whiskey jack daniels tenness...\n",
      "23577                               bj watermelon and mint\n",
      "Name: description, dtype: object\n",
      "\n",
      "Substitutes to \"savile shapoo rizos linaza savile shampoo rizos linaza 25.36 ounce\":\n",
      "4128     gerber 2nd chicken noodle gerber 2nd foods chi...\n",
      "8095           vel vrgn gdlpe vel virgen guadalupe 1 count\n",
      "20055    haagen dazs chocolate haagen dazs chocolate ic...\n",
      "31214                                           taco combo\n",
      "32879    el leoncito trocitos 24pk chaca chaca trocitos...\n",
      "47575    savile shapoo rizos linaza savile shampoo rizo...\n",
      "Name: description, dtype: object\n",
      "\n",
      "Substitutes to \"selecciones\":\n",
      "212           guajes rojos storebrand guajes rojos 1 pound\n",
      "13088        shasta orange 2 lt shasta orange soda 2 liter\n",
      "28426    trocitos de res fresquesitos trocitos de res  ...\n",
      "29787    el mexicano queso frsco csro  tray el mexicano...\n",
      "34630    fud original ham 16 10oz. fud original ham 16 ...\n",
      "36523                                          selecciones\n",
      "Name: description, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    rand_idx = np.random.randint(len(vector_df))\n",
    "    print_recommendations_by_product(nn_matrix, vector_df, ncr_items, rand_idx)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Training the Graph Convolutional Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notebook_image/loss.png\" alt=\"loss\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: \n",
    "# 1. item_id corresponding to node u (scalar) \n",
    "# 2. vector representations of all nodes Z (matrix formatted as a 2D numpy array)\n",
    "# 3. k-means output (df4 above) (pandas dataframe)\n",
    "def loss_function(item_id, Z, df4):\n",
    "    \n",
    "    # step 1: match item_id to row index (u)\n",
    "    u = # not sure how to do this\n",
    "    \n",
    "    # step 2: set alpha \n",
    "    alpha = 0.5\n",
    "\n",
    "    # step 3: go on random walk\n",
    "    # u is the starting node\n",
    "    p = np.zeros(T.shape[0])\n",
    "    p[u] = 1\n",
    "    p = p.reshape(-1,1)\n",
    "    # define the random walk length, say 10\n",
    "    walkLength = 10\n",
    "    weights = []\n",
    "    visited = []\n",
    "    starting_node = u\n",
    "    for k in range(walkLength):\n",
    "        # evaluate the next state vector\n",
    "        p = np.dot(T,p)\n",
    "        # choose the node with higher probability as the visited node\n",
    "        visited.append(np.argmax(p))\n",
    "        weights.append(T[starting_node, np.argmax(p)])\n",
    "        starting_node = np.argmax(p)\n",
    "    # v is the node where you end up at the end of the random walk\n",
    "    v=visited[-1]\n",
    "    # calculate geometric mean of weights along random walk\n",
    "    geometric_mean = stats.gmean(weights)\n",
    "    \n",
    "    # step 4: define sigmoid function\n",
    "    def sigmoid(x):   \n",
    "        z = 1/(1 + np.exp(-x)) \n",
    "    return z\n",
    "\n",
    "    # step 5: calculate first term\n",
    "    # assuming that row u of matrix Z is the vector representation for node u\n",
    "    first_term = -(geometric_mean**alpha)*np.log(sigmoid(np.dot(Z[u,:],Z[v,:])))\n",
    "    \n",
    "    # step 6: calculate second term\n",
    "    # recommended to use Q = 2, 3, 4, or 5 since we have a large dataset\n",
    "    Q = 3 \n",
    "    # get one positive sample and Q negative samples\n",
    "    # positive sample is from u_cluster\n",
    "    # negative samples are not from u_cluster\n",
    "    \n",
    "    # which cluster corresponding to node u?\n",
    "    u_cluster = df4[df4[\"item_id\"] == item_id][\"cluster\"].tolist()[0]\n",
    "    \n",
    "    positive_sampling_df = df4[df4[\"cluster\"] == u_cluster]\n",
    "    row = positive_sampling_df.sample()\n",
    "    positive_item_id = row[\"item_id\"].tolist()[0]\n",
    "    \n",
    "    negative_sampling_df = df4[df4[\"cluster\"] != u_cluster]\n",
    "    rows = negative_sampling_df.sample(n=3, replace=False)\n",
    "    negative_item_ids = rows[\"item_id\"].tolist()\n",
    "    \n",
    "    # need to get corresponding rows of Z to fill in below by matching item_ids to row indices\n",
    "    positive_sample = np.log(sigmoid(np.dot(-Z[u,:],Z[,:])))\n",
    "    negative_sample_1 = np.log(sigmoid(np.dot(-Z[u,:],Z[,:])))\n",
    "    negative_sample_2 = np.log(sigmoid(np.dot(-Z[u,:],Z[,:])))\n",
    "    negative_sample_3 = np.log(sigmoid(np.dot(-Z[u,:],Z[,:])))\n",
    "    \n",
    "    # then calculate expectation (this is just arithmetic mean in this case?)\n",
    "    expectation = (positive_sample+negative_sample_1+negative_sample_2+negative_sample_3)/4\n",
    "    second_term = Q*expectation\n",
    "    \n",
    "    # step 7: subtract second term from first term to get loss\n",
    "    loss = first_term - second_term\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Implementing training via Backpropagation (Future Work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to finalize the GCN would be to actually implement a training function using the Loss Function shown above. The training procedure would utilize the ***Backpropagation Algorithm***, which could be implemented from \"scratch\" using only NumPy/SciPy functions, or with the help of Deep Learning libraries like **Apache MXNet**, **TensorFlow** or **Caffe**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Potential Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some avenues for future work on our implementation are training Word2Vec, using more data, and tuning hyperparameters. We used Gensim's implementation of Word2Vec, which is pre-trained on the Google News dataset. As far as using more data, item descriptions and images could be used. The researchers at Target used these features in their implementation. We only used item names because we did not have item descriptions for most items, and we did not have any item images. If this data becomes available in the future, it can be incorporated into the model easily. Lastly, the following hyperparameters can be tuned: length of item vectors in Word2Vec, length of random walks, number of random walks, number of neighbors in the sampling algorithm, beta in the sampling algorithm, gamma in the aggregation algorithm, alpha in the loss function, number of layers in the network. Perhaps even better substitutes can be obtained by working in some or all of these areas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
